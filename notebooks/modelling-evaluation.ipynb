{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Report-Introduction\" data-toc-modified-id=\"Report-Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Report Introduction</a></span></li><li><span><a href=\"#Background\" data-toc-modified-id=\"Background-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Background</a></span></li><li><span><a href=\"#Modeling-Assumptions-and-Potential-Issues\" data-toc-modified-id=\"Modeling-Assumptions-and-Potential-Issues-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Modeling Assumptions and Potential Issues</a></span></li><li><span><a href=\"#Test-design\" data-toc-modified-id=\"Test-design-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Test design</a></span><ul class=\"toc-item\"><li><span><a href=\"#Creating-the-Testing-and-Training-Datasets\" data-toc-modified-id=\"Creating-the-Testing-and-Training-Datasets-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Creating the Testing and Training Datasets</a></span><ul class=\"toc-item\"><li><span><a href=\"#One-Hot-Encoding-and-Minor-Manipulation\" data-toc-modified-id=\"One-Hot-Encoding-and-Minor-Manipulation-4.1.1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>One Hot Encoding and Minor Manipulation</a></span></li><li><span><a href=\"#Test-Split-and-Scaling\" data-toc-modified-id=\"Test-Split-and-Scaling-4.1.2\"><span class=\"toc-item-num\">4.1.2&nbsp;&nbsp;</span>Test Split and Scaling</a></span></li></ul></li></ul></li><li><span><a href=\"#Models-Description-and-Assessments\" data-toc-modified-id=\"Models-Description-and-Assessments-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Models Description and Assessments</a></span><ul class=\"toc-item\"><li><span><a href=\"#Logistic-Regression\" data-toc-modified-id=\"Logistic-Regression-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Logistic Regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Construction-and-Tuning\" data-toc-modified-id=\"Construction-and-Tuning-5.1.2\"><span class=\"toc-item-num\">5.1.2&nbsp;&nbsp;</span>Construction and Tuning</a></span></li><li><span><a href=\"#Interpretation\" data-toc-modified-id=\"Interpretation-5.1.3\"><span class=\"toc-item-num\">5.1.3&nbsp;&nbsp;</span>Interpretation</a></span></li><li><span><a href=\"#Assessment\" data-toc-modified-id=\"Assessment-5.1.4\"><span class=\"toc-item-num\">5.1.4&nbsp;&nbsp;</span>Assessment</a></span></li></ul></li><li><span><a href=\"#Support-Vector-Machines-(SVM)\" data-toc-modified-id=\"Support-Vector-Machines-(SVM)-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Support Vector Machines (SVM)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-5.2.1\"><span class=\"toc-item-num\">5.2.1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Construction-and-Tuning\" data-toc-modified-id=\"Construction-and-Tuning-5.2.2\"><span class=\"toc-item-num\">5.2.2&nbsp;&nbsp;</span>Construction and Tuning</a></span></li><li><span><a href=\"#Interpretation\" data-toc-modified-id=\"Interpretation-5.2.3\"><span class=\"toc-item-num\">5.2.3&nbsp;&nbsp;</span>Interpretation</a></span></li><li><span><a href=\"#Detailed-assessment\" data-toc-modified-id=\"Detailed-assessment-5.2.4\"><span class=\"toc-item-num\">5.2.4&nbsp;&nbsp;</span>Detailed assessment</a></span></li></ul></li><li><span><a href=\"#Random-Forest\" data-toc-modified-id=\"Random-Forest-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Random Forest</a></span><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-5.3.1\"><span class=\"toc-item-num\">5.3.1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Construction-and-Tuning\" data-toc-modified-id=\"Construction-and-Tuning-5.3.2\"><span class=\"toc-item-num\">5.3.2&nbsp;&nbsp;</span>Construction and Tuning</a></span></li><li><span><a href=\"#Interpretation\" data-toc-modified-id=\"Interpretation-5.3.3\"><span class=\"toc-item-num\">5.3.3&nbsp;&nbsp;</span>Interpretation</a></span></li><li><span><a href=\"#Assessment\" data-toc-modified-id=\"Assessment-5.3.4\"><span class=\"toc-item-num\">5.3.4&nbsp;&nbsp;</span>Assessment</a></span></li></ul></li></ul></li><li><span><a href=\"#Summary-of-Modelling\" data-toc-modified-id=\"Summary-of-Modelling-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Summary of Modelling</a></span></li><li><span><a href=\"#Evaluation-of-Data-Mining-Process\" data-toc-modified-id=\"Evaluation-of-Data-Mining-Process-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Evaluation of Data Mining Process</a></span><ul class=\"toc-item\"><li><span><a href=\"#Business-success-criteria\" data-toc-modified-id=\"Business-success-criteria-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Business success criteria</a></span></li><li><span><a href=\"#Review-of-project-success:\" data-toc-modified-id=\"Review-of-project-success:-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Review of project success:</a></span></li><li><span><a href=\"#Techniques-and-tools\" data-toc-modified-id=\"Techniques-and-tools-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>Techniques and tools</a></span></li><li><span><a href=\"#Future-Work\" data-toc-modified-id=\"Future-Work-7.4\"><span class=\"toc-item-num\">7.4&nbsp;&nbsp;</span>Future Work</a></span></li></ul></li><li><span><a href=\"#References\" data-toc-modified-id=\"References-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>References</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 class=\"tocSkip\"><center class=\"tocSkip\">Modelling and Evaluation Report</center></h1>\n",
    "<h1 class=\"tocSkip\"><center class=\"tocSkip\"><i class=\"tocSkip\">Ammar Hasan 150454388</i></center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "import sklearn.metrics as met\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import time\n",
    "\n",
    "\n",
    "BASE_PROCESSED_DATA_DIR = '../data/processed'\n",
    "\"\"\"\n",
    "str: Base processed data directory\n",
    "\"\"\"\n",
    "\n",
    "PROCESSED_CSV_FILE = BASE_PROCESSED_DATA_DIR + '/processed.csv'\n",
    "\"\"\"\n",
    "str: HAM1000_metadata.csv metadata file location \n",
    "\"\"\"\n",
    "        \n",
    "# Read dataset in\n",
    "skin_df = pd.read_csv(PROCESSED_CSV_FILE, index_col=0)\n",
    "\"\"\"\n",
    "pandas.core.frame.DataFrame: final dataset\n",
    "\"\"\"\n",
    "def tuning(function, parameters, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Used to tune a model using 5 cv\n",
    "    adapted from https://github.com/yuguan1/example-ML-code\n",
    "    \n",
    "    returns best parameters from GridSearchCV\n",
    "    \"\"\"\n",
    "    print(\"# Tuning hyper-parameters\")\n",
    "    clf = GridSearchCV(function, parameters, cv=5)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print('best parameters:')\n",
    "    print(clf.best_params_)\n",
    "    print('-------------------------------------')\n",
    "    return(clf.best_params_)\n",
    "\n",
    "def printMetrics(prediction, y_test):\n",
    "    \"\"\"\n",
    "    Prints accuracy, confusion and F1 metrics\n",
    "    \n",
    "    returns list of accuracy, confusion and F1 metrics\n",
    "    \"\"\"\n",
    "    accuracy = met.accuracy_score(y_test, prediction)\n",
    "    confusion = met.confusion_matrix(y_test, prediction)\n",
    "    f1_score = met.f1_score(y_test, prediction, average='weighted')\n",
    "    \n",
    "    print('accuracy', accuracy)\n",
    "    print()\n",
    "    print(confusion)\n",
    "    print()\n",
    "    print('f1', f1_score)\n",
    "    \n",
    "    return([accuracy, confusion, f1_score])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report Introduction\n",
    "\n",
    "This report documents the Modelling and Evaluation stages of the CRISP-DM process followed by this project. This stage documents the construction of models and their evaluation - and to a lesser extent the project itself. This include background information about the models, their training, assessment and other evaluations. \n",
    "\n",
    "## Background \n",
    "\n",
    "As stated in the Business Understanding report the Pigment Skin Diagnosis field use of technology has been growing due critical importance of early detection, and the computerised detection of Skin Lesions is becoming critical. In this project as stated in the criteria of Business Understanding report, the objective of this project is to develop, train and evaluate models (logistic, SVM, neural) to classify skin lesion types and to compare the different models. \n",
    "\n",
    "## Modeling Assumptions and Potential Issues\n",
    "\n",
    "As stated in the Data Understanding report, some issues with the data were recognised:\n",
    "\n",
    "* Some of the sexes are unknown and the mention of whether these unknown sexes are simply unknown or are non-binary/non-conforming is missing.\n",
    "\n",
    "* Ground truths were obtained by various methods including expert consensus, which might effect the consistency of the classification.\n",
    "\n",
    "Hence, we need to assume that both the unknown sexes and the different methods to obtain the ground truth will not have a major impact on the modelling and its analysis.\n",
    "\n",
    "\n",
    "## Test design\n",
    "\n",
    "As alluded to before in the background, the following classification models will be built in this project: \n",
    "\n",
    "* A neural network - specifically a Convoluted Neural Network \n",
    "\n",
    "* Logistic Regression Model \n",
    "\n",
    "* A Support Vector Machine \n",
    "\n",
    "Models are fitted by using a training data set and if tuning is required 5 fold cross validation is also used to find the optimal parameters. To test models the testing data set is used after the model was trained to evaluate the model (accuracy, confusion, f1 score, etc). The Training and test data is created by splitting the post processed final dataset described in the Data Understanding report in a 50-50 split respectively.\n",
    "\n",
    "### Creating the Testing and Training Datasets\n",
    "\n",
    "The final dataset whose creation was described in the Data Preparation report needs minor changes before the modelling begins. In particular, the categorical variables need to be hot coded and the data needs to be divided into model training and model testing samples.\n",
    "\n",
    "#### One Hot Encoding and Minor Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode categorical cols using one hot encoding\n",
    "\n",
    "one_hot_localization = pd.get_dummies(skin_df['localization'])\n",
    "one_hot_localization.drop('unknown', axis=1, inplace = True)\n",
    "\n",
    "one_hot_sex = pd.get_dummies(skin_df['sex'])\n",
    "one_hot_sex.drop('unknown', axis=1, inplace = True)\n",
    "\n",
    "# Drop old categorical cols and replace with new ones\n",
    "# drop dx type (not needed beyond data understanding)\n",
    "\n",
    "skin_df.drop(['dx_type', 'localization', 'sex'], axis = 1, inplace = True)\n",
    "\n",
    "# Join the encoded dfs\n",
    "\n",
    "skin_df = skin_df.join(one_hot_localization)\n",
    "skin_df = skin_df.join(one_hot_sex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using pandas dummies for categorical variables, localization values are one hot coded using new columns for every value (0 false / 1 true), however one of the columns is dropped a negation of all the other columns represents it. Lastly, the now redundant sex and localization fields are dropped alongside dx_type (no need for analysing diagnosis type beyond Data Understanding).\n",
    "\n",
    "#### Test Split and Scaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and test data in a 50-50 split\n",
    "# Don't include lesion_types (used for response) and image path (not used yet)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    skin_df.drop(['lesion_type_idx', 'lesion_type', 'image_path'], axis=1),\n",
    "    skin_df['lesion_type_idx'], test_size=0.5, random_state=0)\n",
    "\n",
    "# scale using a partial fit for speed\n",
    "\n",
    "scaling = StandardScaler()\n",
    "\n",
    "scaling.partial_fit(X_test)\n",
    "X_test = scaling.transform(X_test)\n",
    "\n",
    "scaling.partial_fit(X_train)\n",
    "X_train = scaling.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data and the testing data are separated using a 50-50 split respectively, both sets consist of a set of predictors (X) and a response (y). The predictor data has the lesion_type_idx and lesion_type fields removed since they can leak the ground truth. For the response data only the lesion_type_idx field is used since it is sufficient at representing the category of skin lesion (the response / what is being predicted).\n",
    "\n",
    "To ensure that the impact of predictors is not effected by the measurement scale - which could occur in this dataset due to the variety of predictors, the predictors are scaled using a scaling transform (i.e. with default mean and standard deviation).\n",
    "\n",
    "### Measurements\n",
    "\n",
    "The computer used to carry the measurements has the following specifications:\n",
    "* CPU: i7-7700HQ\n",
    "* RAM: 8GB\n",
    "* OS: Windows 10\n",
    "* GPU: GTX 1060 (notebook)\n",
    "\n",
    "To evaluate the model the following measurements are taken:\n",
    "* Fit time: Using the time python library, a timer is started and stopped to measure tuning and fit \n",
    "* Prediction time: Using the time python library, a timer is started and stopped to measure the prediction\n",
    "* Confusion matrix: Using the sklearn metrics library a confusion matrix is printed\n",
    "* F1 Score: Using the sklearn metrics library a F1 score is calculated using weighted averages\n",
    "\n",
    "\n",
    "## Models Description and Assessments\n",
    "\t\t\t\t\n",
    "### Logistic Regression \n",
    "\n",
    "#### Introduction \t\t\n",
    "\n",
    "To achieve its classification, Logistic Regression fits a line to separate the data into classes. The line is fitted by minimising the error between line and points by changing the coefficients/weights and the intercept to find the \"ideal fit\". To minimise the error gradient descent (i.e. loss function optimisation) is used, which updates the parameters (i.e. through partial derivatives) to find a local minimum/maximum. (James et al., 2013)\n",
    "\n",
    "#### Construction and Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# fit with time measurements, use n_jobs -1 for all cores\n",
    "\n",
    "log_fit_start = time.time()\n",
    "clf = LogisticRegression(solver='saga', n_jobs = -1).fit(X_train, y_train)\n",
    "log_fit_end = time.time()\n",
    "print('Logistic fit time: ', log_fit_end - log_fit_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previously stated, logistic regression fits a line through error minimisation and does not require tuning as the objective is straightforward. The model is simply constructed using a saga solver deployed to 4 threads using sklearn's logistic regression model.\n",
    "\n",
    "Nonetheless, the constructed model does not seem to converge, this is not that surprising as the fitted line can have many solutions, and sometimes with a large number of predictors this can cause logistic regression to struggle (James et al., 2013).\n",
    "\n",
    "#### Interpretation\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    0         0         1         2         3         4  \\\n",
      "3136              age  0.177739  0.344569  0.576867 -0.050625 -1.046004   \n",
      "3137          abdomen -0.048107 -0.029776 -0.058278 -0.006311  0.122325   \n",
      "3138            acral -0.021233 -0.017005 -0.056509 -0.012900  0.084261   \n",
      "3139             back -0.097517  0.142206 -0.011433 -0.060731 -0.023215   \n",
      "3140            chest -0.026518  0.136832  0.064297 -0.026464 -0.097892   \n",
      "3141              ear  0.002943 -0.045324 -0.065051 -0.015759 -0.057547   \n",
      "3142             face  0.248112  0.107359  0.314902 -0.050211 -0.623524   \n",
      "3143             foot -0.068091 -0.082843 -0.227369 -0.010350  0.272362   \n",
      "3144          genital -0.024996 -0.024900  0.014478  0.012730  0.044501   \n",
      "3145             hand  0.034016 -0.073001  0.044681 -0.029879  0.041418   \n",
      "3146  lower extremity  0.060830 -0.093954 -0.023619  0.148781 -0.052682   \n",
      "3147             neck  0.014512  0.058319  0.089373 -0.015234 -0.143925   \n",
      "3148            scalp  0.078217 -0.008235  0.023001 -0.044240 -0.062913   \n",
      "3149            trunk -0.111776 -0.037853  0.047825 -0.042882  0.083351   \n",
      "3150  upper extremity  0.101625 -0.036297 -0.060975  0.074524 -0.128183   \n",
      "3151           female -0.064426  0.013299  0.041194 -0.001051 -0.052453   \n",
      "3152             male  0.082280  0.012014 -0.039063  0.013450 -0.066789   \n",
      "\n",
      "             5         6  \n",
      "3136  0.420242 -0.040422  \n",
      "3137 -0.074061  0.055629  \n",
      "3138 -0.038360 -0.007417  \n",
      "3139  0.119100 -0.019940  \n",
      "3140  0.031604 -0.009441  \n",
      "3141  0.111765 -0.008644  \n",
      "3142  0.031893  0.012542  \n",
      "3143  0.022407 -0.046580  \n",
      "3144 -0.105707 -0.034593  \n",
      "3145 -0.074338  0.003625  \n",
      "3146  0.056120 -0.021943  \n",
      "3147  0.068050  0.018954  \n",
      "3148 -0.009782  0.010888  \n",
      "3149 -0.144509  0.077813  \n",
      "3150  0.186383 -0.016310  \n",
      "3151 -0.010326  0.032992  \n",
      "3152  0.079878 -0.014560  \n"
     ]
    }
   ],
   "source": [
    "# form table for coefficients using column names\n",
    "\n",
    "coefs = pd.concat([pd.DataFrame(skin_df.drop(['lesion_type_idx','lesion_type','image_path'], axis = 1).columns),\n",
    "                   pd.DataFrame(np.transpose(clf.coef_))], axis = 1)\n",
    "                 \n",
    "# print table\n",
    "                   \n",
    "print(coefs.iloc[3136:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since as described in the introduction of logistic regression, logistic regression uses a set of coefficients and weights to draw the classification line, it is easy to interpret the model by examining them. However, since there are many predictors we only look at other meta data that is not directly related to the pixels.\n",
    "\n",
    "Here it can be seen that age is the strongest predictor, in particular with classes 1, 2 and 4 (Basal cell carcinoma, Benign keratosis and Melanocytic nevi respectively), where in classes 1 and 2 age had a positive impact, but in class 4 age had a strong negative one, some of these classes were predicted to have a strong dependence on age in the Data Understanding stage. Age seems to mostly have a positive trend nonetheless - which matches with the observations in the data understanding stage.\n",
    "\n",
    "When it comes to sex, three classes come to attention, classes 0, 4 and 5 (Actinic keratoses, Melanocytic nevi and melanoma respectively). In class 0, males have a noticiable positive impact which is also seen with class . In class 4 females have a noticiable negative impact. However, in comparison to age, the sexes impact is minor.\n",
    "\n",
    "For localization, the face seems to have a strong impact when it comes to class 0  (Actinic keratoses), also in class 2 ((Basal cell carcinoma), the face also has a strong impact on a classification in this class while the foot has the opposite effect. However, the classes mostly effected by localization are classes 4 (Melanocytic nevi) where a localization in the face very strongly suggests against a classification to class 4. Overall, localization is the second strongest predictor here.\n",
    "\n",
    "#### Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7328274760383386\n",
      "\n",
      "[[  32   24   60    0   32    9    0]\n",
      " [  14  106   56    0   74    9    0]\n",
      " [   8   28  243    0  193   65    0]\n",
      " [   5   13    4    1   38    3    0]\n",
      " [   6   23   82    0 3151   94    0]\n",
      " [   7   14   97    0  310  132    1]\n",
      " [   0   14    5    0   46    4    5]]\n",
      "\n",
      "f1 [0.27947598 0.44074844 0.44833948 0.03076923 0.87527778 0.30102623\n",
      " 0.125     ]\n"
     ]
    }
   ],
   "source": [
    "# carry prediction with time measurements \n",
    "# while recording prediction\n",
    "\n",
    "log_pred_start = time.time()\n",
    "prediction = clf.predict(X_test)\n",
    "log_pred_end = time.time()\n",
    "\n",
    "log_met = printMetrics(prediction, y_test)\n",
    "print('Logistic prediction time: ', log_pred_end - log_pred_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that while the overall accuracy is good, the F1 scores and poor except for class 5 (), this is worrying because as stated before in the business understanding false negative performance is important, and a poor F1 suggests a poor false negative performance as can be seen in the confusion table.\n",
    "\n",
    "This could be because of the issue observed in the model construction, as there was no convergence due to the large number of predictors perhaps. This might be fixed with the introduction of kernels as explained in the next section.\n",
    "\n",
    "### Support Vector Machines (SVM)\n",
    "\n",
    "#### Introduction \t\n",
    "\n",
    "Support Vector Machines are another classification model, which is an improvement logistic regression through the addition of two margins for the line which all class points must be behind and the distance to them from the line maximised (again using gradient descent). This ensures that the fairest line out of a set of lines that separate the points is chosen. (Rieck et al., 2012)\n",
    "\n",
    "To help interpreting non linear data for SVMs, kernels (commonly a Raidial Bias Function (RBF)) transform the data to a higher dimension (kernel trick) (Rieck et al., 2012).\n",
    "\n",
    "#### Construction and Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit\n",
    "\n",
    "svm_fit_start = time.time()\n",
    "\n",
    "n_estimators = 10\n",
    "clf = OneVsRestClassifier(\n",
    "    BaggingClassifier(\n",
    "        SVC(kernel='rbf', random_state=0),\n",
    "        max_samples=1.0 / n_estimators, n_jobs=-1,\n",
    "        n_estimators=n_estimators, random_state=0\n",
    "    ), n_jobs=-1\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "svm_fit_start = time.time()\n",
    "print('SVM fit time: ', svm_fit_end - svm_fit_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to their high complexity (more on that later) a Bagging Classifier with a One Vs Rest Classifier (OvR) is used to help speed up the execution. The OvR basically creates a classifier for every class of a multi class classification problem to improve efficiency - enables paralysation - (“sklearn.multiclass.OneVsRestClassifier — scikit-learn 0.20.2 documentation,” n.d.).\n",
    "\n",
    "On the other hand, the Bagging Classifiers which are used by the OneVsRestClassifier works like a Random Decision tree, as it fits multiple classifiers on random subsets of the original dataset which it then aggregates for the final decision (“sklearn.ensemble.BaggingClassifier — scikit-learn 0.20.2 documentation,” n.d.). This ensemble approach reduces the size of the dataset and enables paralysation.\n",
    "\n",
    "However, this significantly complicates the function and hence tuning is not possible using a GridSearchCV for instance.\n",
    "\n",
    "#### Interpretation\t\n",
    "\n",
    "Support Vector Machine are more black box like due to their use of kernels, and hence difficult to interpret when it comes to predictor influence - the transformation into higher dimensions makes it difficult to find coefficients. \n",
    "\n",
    "#### Detailed assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'confusion_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-106b57ac8ec7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msvm_pred_end\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mprintMetrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-25-abb6f8010534>\u001b[0m in \u001b[0;36mprintMetrics\u001b[1;34m(prediction, y_test)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \"\"\"\n\u001b[0;32m     54\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m     \u001b[0mconfusion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m     \u001b[0mf1_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'macro'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'confusion_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "# Prediction \n",
    "\n",
    "svm_pred_start = time.time()\n",
    "prediction = clf.predict(X_test)\n",
    "svm_pred_end = time.time()\n",
    "\n",
    "svm_met = printMetrics(prediction, y_test)\n",
    "print('SVM prediction time: ', svm_pred_end - svm_pred_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Random Forest \n",
    "\n",
    "#### Introduction \t\t\n",
    "\n",
    "A random forest is a modelling technique that makes use of meta estimation through the use of multiple decision trees whose results are combined to form a final decision. The reliance on multiple decision trees helps resolve issues such as overfitting - reduces variance for bias. (“3.2.4.3.1. sklearn.ensemble.RandomForestClassifier — scikit-learn 0.20.2 documentation,” n.d.)\n",
    "\n",
    "The decision trees themselves are simple non-parametric supervised learning classifiers that make decisions throughout a structure based on entropy of values - high entropy branches and nodes are at higher levels. (“1.10. Decision Trees — scikit-learn 0.20.2 documentation,” n.d.)\n",
    "\n",
    "\n",
    "#### Construction and Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_fit_start = time.time()\n",
    "\n",
    "parameters = [{'max_depth': [8,30,45],\n",
    "               'n_estimators': [150, 450, 550]}]\n",
    "\n",
    "# Tune parameters using 5 fold cross validation \n",
    "best_params = tuning(RandomForestClassifier(n_jobs = -1, random_state=0), parameters, X_train, y_train)\n",
    "\n",
    "\n",
    "optimalDepth = best_params['max_depth']\n",
    "optimalEstimators = best_params['n_estimators']\n",
    "clf = RandomForestClassifier(random_state=0, max_depth = optimalDepth, n_estimators = optimalEstimators)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "for_fit_end = time.time()\n",
    "print('Random Forest fit time: ', for_fit_end - for_fit_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forest Classifier is tuned using a GridSearchCV on two parameters, 'max_depth' and 'n_estimators'. 'max_depth' controls how deep the tree can be constructed - reducing overfitting and simplifies model - and 'n_estimators' controls the number of trees that are created - cuts overfitting for underfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation\t\t\t\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = pd.concat(\n",
    "    [pd.DataFrame(skin_df.drop(['lesion_type_idx', 'lesion_type'], axis = 1).columns),\n",
    "                  pd.DataFrame(np.transpose(clf.feature_importances_))], axis = 1)\n",
    "print(importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(pjh2011, 2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction and Assesmenmt\n",
    "\n",
    "for_pred_start = time.time()\n",
    "prediction = clf.predict(X_test)\n",
    "for_pred_end = time.time()\n",
    "\n",
    "for_met = printMetrics(prediction, y_test)\n",
    "print('Random Forest prediction time: ', for_pred_end - for_pred_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Modelling\n",
    "\n",
    "### Fitting Times\n",
    "\n",
    "### Prediction Times\n",
    "\n",
    "### Accuracy \n",
    "\n",
    "### F1 Scores \n",
    "\n",
    "\n",
    "## Evaluation of Data Mining Process  \n",
    "\n",
    "\n",
    "### Business success criteria\n",
    "\t\t\t\t\n",
    "### Review of project success: \n",
    "\n",
    "### Techniques and tools \n",
    "\n",
    "### Future Work \n",
    "\n",
    "\n",
    "## References \n",
    "\n",
    "1. James, G., Witten, D., Hastie, T., Tibshirani, R., 2013. Classification, in: James, G., Witten, D., Hastie, T., Tibshirani, R. (Eds.), An Introduction to Statistical Learning: With Applications in R, Springer Texts in Statistics. Springer New York, New York, NY, pp. 127–173. https://doi.org/10.1007/978-1-4614-7138-7_4\n",
    "\n",
    "2. Rieck, K., Sonnenburg, S., Mika, S., Schäfer, C., Laskov, P., Tax, D., Müller, K.-R., 2012. Support Vector Machines, in: Gentle, J.E., Härdle, W.K., Mori, Y. (Eds.), Handbook of Computational Statistics: Concepts and Methods, Springer Handbooks of Computational Statistics. Springer Berlin Heidelberg, Berlin, Heidelberg, pp. 883–926. https://doi.org/10.1007/978-3-642-21551-3_30\n",
    "\n",
    "3. sklearn.multiclass.OneVsRestClassifier — scikit-learn 0.20.2 documentation [WWW Document], n.d. URL https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html (accessed 1.24.19).\n",
    "\n",
    "4. sklearn.ensemble.BaggingClassifier — scikit-learn 0.20.2 documentation [WWW Document], n.d. URL https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html (accessed 1.24.19).\n",
    "\n",
    "5. 3.2.4.3.1. sklearn.ensemble.RandomForestClassifier — scikit-learn 0.20.2 documentation [WWW Document], n.d. URL https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html (accessed 1.24.19).\n",
    "\n",
    "6. 1.10. Decision Trees — scikit-learn 0.20.2 documentation [WWW Document], n.d. URL https://scikit-learn.org/stable/modules/tree.html#tree-classification (accessed 1.24.19).\n",
    "\n",
    "7. pjh2011, 2018. Contribute to pjh2011/rf_perm_feat_import development by creating an account on GitHub. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
