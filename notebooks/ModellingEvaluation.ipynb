{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Report-Introduction\" data-toc-modified-id=\"Report-Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Report Introduction</a></span></li><li><span><a href=\"#Background\" data-toc-modified-id=\"Background-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Background</a></span></li><li><span><a href=\"#Modeling-Assumptions-and-Potential-Issues\" data-toc-modified-id=\"Modeling-Assumptions-and-Potential-Issues-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Modeling Assumptions and Potential Issues</a></span></li><li><span><a href=\"#Test-design\" data-toc-modified-id=\"Test-design-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Test design</a></span><ul class=\"toc-item\"><li><span><a href=\"#Creating-the-Testing-and-Training-Datasets\" data-toc-modified-id=\"Creating-the-Testing-and-Training-Datasets-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Creating the Testing and Training Datasets</a></span></li></ul></li><li><span><a href=\"#Models-Description-and-Assessments\" data-toc-modified-id=\"Models-Description-and-Assessments-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Models Description and Assessments</a></span><ul class=\"toc-item\"><li><span><a href=\"#Logistic-Regression\" data-toc-modified-id=\"Logistic-Regression-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Logistic Regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Construction-and-Tuning\" data-toc-modified-id=\"Construction-and-Tuning-5.1.2\"><span class=\"toc-item-num\">5.1.2&nbsp;&nbsp;</span>Construction and Tuning</a></span></li><li><span><a href=\"#Interpretation\" data-toc-modified-id=\"Interpretation-5.1.3\"><span class=\"toc-item-num\">5.1.3&nbsp;&nbsp;</span>Interpretation</a></span></li><li><span><a href=\"#Detailed-assessment\" data-toc-modified-id=\"Detailed-assessment-5.1.4\"><span class=\"toc-item-num\">5.1.4&nbsp;&nbsp;</span>Detailed assessment</a></span></li></ul></li><li><span><a href=\"#Support-Vector-Machines-(SVM)\" data-toc-modified-id=\"Support-Vector-Machines-(SVM)-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Support Vector Machines (SVM)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-5.2.1\"><span class=\"toc-item-num\">5.2.1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Construction-and-Tuning\" data-toc-modified-id=\"Construction-and-Tuning-5.2.2\"><span class=\"toc-item-num\">5.2.2&nbsp;&nbsp;</span>Construction and Tuning</a></span></li><li><span><a href=\"#Interpretation\" data-toc-modified-id=\"Interpretation-5.2.3\"><span class=\"toc-item-num\">5.2.3&nbsp;&nbsp;</span>Interpretation</a></span></li><li><span><a href=\"#Detailed-assessment\" data-toc-modified-id=\"Detailed-assessment-5.2.4\"><span class=\"toc-item-num\">5.2.4&nbsp;&nbsp;</span>Detailed assessment</a></span></li></ul></li><li><span><a href=\"#Convoluted-Neural-Network-(CNN)\" data-toc-modified-id=\"Convoluted-Neural-Network-(CNN)-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Convoluted Neural Network (CNN)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-5.3.1\"><span class=\"toc-item-num\">5.3.1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Construction-and-Tuning\" data-toc-modified-id=\"Construction-and-Tuning-5.3.2\"><span class=\"toc-item-num\">5.3.2&nbsp;&nbsp;</span>Construction and Tuning</a></span></li><li><span><a href=\"#Interpretation\" data-toc-modified-id=\"Interpretation-5.3.3\"><span class=\"toc-item-num\">5.3.3&nbsp;&nbsp;</span>Interpretation</a></span></li><li><span><a href=\"#Detailed-assessment\" data-toc-modified-id=\"Detailed-assessment-5.3.4\"><span class=\"toc-item-num\">5.3.4&nbsp;&nbsp;</span>Detailed assessment</a></span></li></ul></li></ul></li><li><span><a href=\"#Summary-of-Modelling\" data-toc-modified-id=\"Summary-of-Modelling-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Summary of Modelling</a></span></li><li><span><a href=\"#Evaluation-of-Data-Mining-Process\" data-toc-modified-id=\"Evaluation-of-Data-Mining-Process-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Evaluation of Data Mining Process</a></span><ul class=\"toc-item\"><li><span><a href=\"#Business-success-criteria\" data-toc-modified-id=\"Business-success-criteria-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Business success criteria</a></span></li><li><span><a href=\"#Review-of-project-success:\" data-toc-modified-id=\"Review-of-project-success:-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Review of project success:</a></span></li><li><span><a href=\"#Techniques-and-tools\" data-toc-modified-id=\"Techniques-and-tools-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>Techniques and tools</a></span></li><li><span><a href=\"#Future-Work\" data-toc-modified-id=\"Future-Work-7.4\"><span class=\"toc-item-num\">7.4&nbsp;&nbsp;</span>Future Work</a></span></li></ul></li><li><span><a href=\"#References\" data-toc-modified-id=\"References-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>References</a></span></li><li><span><a href=\"#Background\" data-toc-modified-id=\"Background-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Background</a></span></li><li><span><a href=\"#Modeling-assumptions\" data-toc-modified-id=\"Modeling-assumptions-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Modeling assumptions</a></span></li><li><span><a href=\"#Test-design\" data-toc-modified-id=\"Test-design-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Test design</a></span></li><li><span><a href=\"#Model-description-and-assessment\" data-toc-modified-id=\"Model-description-and-assessment-12\"><span class=\"toc-item-num\">12&nbsp;&nbsp;</span>Model description and assessment</a></span><ul class=\"toc-item\"><li><span><a href=\"#Overview-of-models-produced.\" data-toc-modified-id=\"Overview-of-models-produced.-12.1\"><span class=\"toc-item-num\">12.1&nbsp;&nbsp;</span>Overview of models produced.</a></span><ul class=\"toc-item\"><li><span><a href=\"#Type-of-model-and-relationship-to-data-mining-goals\" data-toc-modified-id=\"Type-of-model-and-relationship-to-data-mining-goals-12.1.1\"><span class=\"toc-item-num\">12.1.1&nbsp;&nbsp;</span>Type of model and relationship to data mining goals</a></span></li><li><span><a href=\"#Tuning-and-parameters\" data-toc-modified-id=\"Tuning-and-parameters-12.1.2\"><span class=\"toc-item-num\">12.1.2&nbsp;&nbsp;</span>Tuning and parameters</a></span></li><li><span><a href=\"#Detailed-description-of-the-model-and-any-special-features.\" data-toc-modified-id=\"Detailed-description-of-the-model-and-any-special-features.-12.1.3\"><span class=\"toc-item-num\">12.1.3&nbsp;&nbsp;</span>Detailed description of the model and any special features.</a></span></li></ul></li><li><span><a href=\"#Overview-of-assessment-process-and-results,-including-any-deviations-from-the-test-plan.-For-each-model:\" data-toc-modified-id=\"Overview-of-assessment-process-and-results,-including-any-deviations-from-the-test-plan.-For-each-model:-12.2\"><span class=\"toc-item-num\">12.2&nbsp;&nbsp;</span>Overview of assessment process and results, including any deviations from the test plan. For each model:</a></span><ul class=\"toc-item\"><li><span><a href=\"#Detailed-assessment,-including-measurements-such-as-accuracy-and-interpretation-of-behavior\" data-toc-modified-id=\"Detailed-assessment,-including-measurements-such-as-accuracy-and-interpretation-of-behavior-12.2.1\"><span class=\"toc-item-num\">12.2.1&nbsp;&nbsp;</span>Detailed assessment, including measurements such as accuracy and interpretation of behavior</a></span></li><li><span><a href=\"#Insights-into-why-a-certain-modeling-technique-and-certain-parameter-settings-led-to-good/bad-results\" data-toc-modified-id=\"Insights-into-why-a-certain-modeling-technique-and-certain-parameter-settings-led-to-good/bad-results-12.2.2\"><span class=\"toc-item-num\">12.2.2&nbsp;&nbsp;</span>Insights into why a certain modeling technique and certain parameter settings led to good/bad results</a></span></li><li><span><a href=\"#Conclusions-regarding-patterns-in-the-data-(interpretation)\" data-toc-modified-id=\"Conclusions-regarding-patterns-in-the-data-(interpretation)-12.2.3\"><span class=\"toc-item-num\">12.2.3&nbsp;&nbsp;</span>Conclusions regarding patterns in the data (interpretation)</a></span></li></ul></li></ul></li><li><span><a href=\"#Summary-of-conclusions\" data-toc-modified-id=\"Summary-of-conclusions-13\"><span class=\"toc-item-num\">13&nbsp;&nbsp;</span>Summary of conclusions</a></span></li><li><span><a href=\"#Assessment-of-data-mining-results-with-respect-to-business-success-criteria\" data-toc-modified-id=\"Assessment-of-data-mining-results-with-respect-to-business-success-criteria-14\"><span class=\"toc-item-num\">14&nbsp;&nbsp;</span>Assessment of data mining results with respect to business success criteria</a></span><ul class=\"toc-item\"><li><span><a href=\"#Review-of-business-objectives-and-business-success-criteria-(which-may-have-changed-during-and/or-as-a-result-of-data-mining).\" data-toc-modified-id=\"Review-of-business-objectives-and-business-success-criteria-(which-may-have-changed-during-and/or-as-a-result-of-data-mining).-14.1\"><span class=\"toc-item-num\">14.1&nbsp;&nbsp;</span>Review of business objectives and business success criteria (which may have changed during and/or as a result of data mining).</a></span><ul class=\"toc-item\"><li><span><a href=\"#For-each-business-success-criterion:\" data-toc-modified-id=\"For-each-business-success-criterion:-14.1.1\"><span class=\"toc-item-num\">14.1.1&nbsp;&nbsp;</span>For each business success criterion:</a></span></li></ul></li><li><span><a href=\"#Review-of-project-success:\" data-toc-modified-id=\"Review-of-project-success:-14.2\"><span class=\"toc-item-num\">14.2&nbsp;&nbsp;</span>Review of project success:</a></span><ul class=\"toc-item\"><li><span><a href=\"#Has-the-project-achieved-the-original-business-objectives?\" data-toc-modified-id=\"Has-the-project-achieved-the-original-business-objectives?-14.2.1\"><span class=\"toc-item-num\">14.2.1&nbsp;&nbsp;</span>Has the project achieved the original business objectives?</a></span></li><li><span><a href=\"#Are-there-new-business-objectives-to-be-addressed-later-in-the-project-or-in-new-projects?\" data-toc-modified-id=\"Are-there-new-business-objectives-to-be-addressed-later-in-the-project-or-in-new-projects?-14.2.2\"><span class=\"toc-item-num\">14.2.2&nbsp;&nbsp;</span>Are there new business objectives to be addressed later in the project or in new projects?</a></span></li></ul></li><li><span><a href=\"#Review-of-process\" data-toc-modified-id=\"Review-of-process-14.3\"><span class=\"toc-item-num\">14.3&nbsp;&nbsp;</span>Review of process</a></span></li><li><span><a href=\"#List-of-possible-actions:-make-recommendations-regarding-the-next-steps-in-the-project.\" data-toc-modified-id=\"List-of-possible-actions:-make-recommendations-regarding-the-next-steps-in-the-project.-14.4\"><span class=\"toc-item-num\">14.4&nbsp;&nbsp;</span>List of possible actions: make recommendations regarding the next steps in the project.</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 class=\"tocSkip\"><center class=\"tocSkip\">Modelling and Evaluation Report</center></h1>\n",
    "<h1 class=\"tocSkip\"><center class=\"tocSkip\"><i class=\"tocSkip\">Ammar Hasan 150454388</i></center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import classification_report\n",
    "import sklearn.metrics as met\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "BASE_PROCESSED_DATA_DIR = '../data/processed'\n",
    "\"\"\"\n",
    "str: Base processed data directory\n",
    "\"\"\"\n",
    "\n",
    "PROCESSED_CSV_FILE = BASE_PROCESSED_DATA_DIR + '/processed.csv'\n",
    "\"\"\"\n",
    "str: HAM1000_metadata.csv metadata file location \n",
    "\"\"\"\n",
    "        \n",
    "# Read datasets in\n",
    "skin_df = pd.read_csv(PROCESSED_CSV_FILE, index_col=0)\n",
    "\n",
    "# Code adapted from https://github.com/yuguan1/example-ML-code\n",
    "## tuning method (5 cv)\n",
    "def tuning(function, parameters, X_train, y_train):\n",
    "    print(\"# Tuning hyper-parameters\")\n",
    "    clf = GridSearchCV(function, parameters, cv=5)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print('best parameters:')\n",
    "    print(clf.best_params_)\n",
    "    print('-------------------------------------')\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "\n",
    "## print metrics (Accuracy, confusion, f1)\n",
    "def printMetrics(prediction, y_test):\n",
    "    print('accuracy', met.accuracy_score(y_test, prediction))\n",
    "    print()\n",
    "    print(met.confusion_matrix(y_test, prediction))\n",
    "    print()\n",
    "    print('f1', met.f1_score(y_test, prediction, average=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report Introduction\n",
    "\n",
    "This report documents the Modelling and Evaluation stages of the CRISP-DM process followed by this project. This stage documents the construction of models and their evaluation - and to a lesser extent the project itself. This include background information about the models, their training, assessment and other evaluations. \n",
    "\n",
    "## Background \n",
    "\n",
    "As stated in the Business Understanding report the Pigment Skin Diagnosis field use of technology has been growing due critical importance of early detection, and the computerised detection of Skin Lesions is becoming critical. In this project as stated in the criteria of Business Understanding report, the objective of this project is to develop, train and evaluate models (logistic, SVM, neural) to classify skin lesion types and to compare the different models. \n",
    "\n",
    "## Modeling Assumptions and Potential Issues\n",
    "\n",
    "As stated in the Data Understanding report, some issues with the data were recognised:\n",
    "\n",
    "* Some of the sexes are unknown and the mention of whether these unknown sexes are simply unknown or are non-binary/non-conforming is missing.\n",
    "\n",
    "* Ground truths were obtained by various methods including expert consensus, which might effect the consistency of the classification.\n",
    "\n",
    "Hence, we need to assume that both the unknown sexes and the different methods to obtain the ground truth will not have a major impact on the modelling and its analysis.\n",
    "\n",
    "\n",
    "## Test design\n",
    "\n",
    "As alluded to before in the background, the following classification models will be built in this project: \n",
    "\n",
    "* A neural network - specifically a Convoluted Neural Network \n",
    "\n",
    "* Logistic Regression Model \n",
    "\n",
    "* A Support Vector Machine \n",
    "\n",
    "Models are fitted by using cross validation (10 fold) and if tuning is required 10 fold cross validation is also used to find the optimal parameters. To test models. \n",
    "\n",
    "The data used for training and testing is provided from the post process metadata dataset described in the Data Understanding report. \n",
    "\n",
    "### Creating the Testing and Training Datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\t\t\t\t\t\t\t\t\n",
    "## Models Description and Assessments\n",
    "\t\t\t\t\n",
    "### Logistic Regression \n",
    "\n",
    "#### Introduction \t\t\n",
    "\n",
    "To achieve its classification, Logistic Regression fits a line to separate the data into classes. The line is fitted by minimising the error between line and points by changing the coefficients/weights and the intercept to find the \"ideal fit\". To minimise the error gradient descent (i.e. loss function optimisation) is used, which updates the parameters (i.e. through partial derivatives) to find a local minimum/maximum. (James et al., 2013)\n",
    "\n",
    "#### Construction and Tuning \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode categorical cols using one hot encoding\n",
    "\n",
    "one_hot_localization = pd.get_dummies(skin_df['localization'])\n",
    "one_hot_localization.drop('neck', axis=1, inplace = True)\n",
    "\n",
    "one_hot_sex = pd.get_dummies(skin_df['sex'])\n",
    "one_hot_sex.drop('unknown', axis=1, inplace = True)\n",
    "\n",
    "# Drop old categorical cols and replace with new ones\n",
    "# drop dx type\n",
    "\n",
    "skin_df.drop(['dx_type', 'localization', 'sex'], axis = 1, inplace = True)\n",
    "\n",
    "# Join the encoded dfs\n",
    "skin_df = skin_df.join(one_hot_localization)\n",
    "skin_df = skin_df.join(one_hot_sex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "skin_df.drop(['lesion_id', 'image_id', 'label_rgb_28_28', 'label_l_28_28', 'image_path', 'dx'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skin_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# standarise\\n\\nX_train.loc[:, X_train.columns != 'image_path'] = StandardScaler().fit_transform(\\n    X_train.loc[:, X_train.columns != 'image_path'])\\n\\nX_test.loc[:, X_test.columns != 'image_path'] = StandardScaler().fit_transform(\\n    X_test.loc[:, X_test.columns != 'image_path'])\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_df = np.array_split(skin_df, 2)\n",
    "\n",
    "X_train = split_df[0].loc[:, ~split_df[0].columns.isin(['lesion_type_idx', 'lesion_type'])]\n",
    "X_test = split_df[1].loc[:, ~split_df[1].columns.isin(['lesion_type_idx', 'lesion_type'])]\n",
    "\n",
    "y_train = split_df[0]['lesion_type_idx']\n",
    "y_test = split_df[1]['lesion_type_idx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling = StandardScaler()\n",
    "scaling.partial_fit(X_test)\n",
    "X_test = scaling.transform(X_test)\n",
    "\n",
    "scaling.partial_fit(X_train)\n",
    "X_train = scaling.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.378070701018574\n",
      "\n",
      "[[   0  135  144    4   11    1   32]\n",
      " [   0    0    0    0    0    0    0]\n",
      " [   0    0    1    0    0    0    3]\n",
      " [   0    0    0    0    0    0    0]\n",
      " [   0  258  760   33 1889   78 1654]\n",
      " [   0    0    0    0    0    0    0]\n",
      " [   0    1    0    0    0    0    3]]\n",
      "\n",
      "f1 [0.         0.         0.00220022 0.         0.57486306 0.\n",
      " 0.00353774]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Fit and prediction\n",
    "clf = LogisticRegression(solver='sag', n_jobs = 6).fit(X_train, y_train)\n",
    "prediction = clf.predict(X_test)\n",
    "printMetrics(prediction, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters\n",
      "best parameters:\n",
      "{'max_depth': 10, 'n_estimators': 200}\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "parameters = [{'max_depth': [4,6,8,10],\n",
    "               'n_estimators': [10,40,100,200]}]\n",
    "\n",
    "# Tune parameters using 5 fold cross validation \n",
    "tuning(RandomForestClassifier(random_state = 0), parameters, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-1c0df0824988>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m importances = pd.concat([pd.DataFrame(X_train.columns),\n\u001b[0m\u001b[0;32m      9\u001b[0m                          pd.DataFrame(np.transpose(clf.feature_importances_))],\n\u001b[0;32m     10\u001b[0m                        axis = 1)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "# fit and prediction\n",
    "optimalDepth = 10\n",
    "optimalEstimators = 200\n",
    "clf = RandomForestClassifier(random_state = 0, max_depth = optimalDepth, n_estimators = optimalEstimators)\n",
    "clf.fit(X_train, y_train)\n",
    "prediction = clf.predict(X_test)\n",
    "\n",
    "importances = pd.concat([pd.DataFrame(X_train.columns),\n",
    "                         pd.DataFrame(np.transpose(clf.feature_importances_))],\n",
    "                       axis = 1)\n",
    "print(importances)\n",
    "# Metrics (predtion vs Test response)\n",
    "printMetrics(prediction, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.372079089275015\n",
      "\n",
      "[[   0  109  147    1   39   31]\n",
      " [   0    0    0    0    0    0]\n",
      " [   0    0    3    0    0    1]\n",
      " [   0    0    0    0    0    0]\n",
      " [   0  301  714    1 1856 1800]\n",
      " [   0    0    0    0    0    4]]\n",
      "\n",
      "f1 [0.         0.         0.00691244 0.         0.56525049 0.00434783]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "printMetrics(prediction, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning\n",
    "parameters = [{'alpha': \n",
    "               [1e-5, 1e-4, 0.01, 0.1, 1.0, 10, 100, 1000]}]\n",
    "\n",
    "# Tune parameters using 5 fold cross validation \n",
    "tuning(Lasso(normalize=False, max_iter=2000, tol=0.01), parameters, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [{'kernel': ['rbf'],\n",
    "                'gamma': [1e-4, 0.01, 0.1, 0.5],\n",
    "                'C': [1, 10, 100, 1000]}]\n",
    "\n",
    "# Tune parameters using 5 fold cross validation \n",
    "tuning(SVC(), parameters, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and prediction\n",
    "clf = Lasso().fit(\n",
    "    X_train.loc[:, X_train.columns != 'image_path'], y_train)\n",
    "prediction = clf.predict(X_test.loc[:, X_train.columns != 'image_path'])nmb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics (predtion vs Test response)\n",
    "printMetrics(prediction, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As previously stated, logistic regression fits a line through error minimisation and does not require tuning as the objective is straightforward. \n",
    "\n",
    "#### Interpretation\t\t\n",
    "\n",
    "#### Detailed assessment\n",
    "\t\t\t\t\t\t\t\t\t\t \n",
    "### Support Vector Machines (SVM)\n",
    "\n",
    "#### Introduction \t\n",
    "\n",
    "Support Vector Machines are another classification model, which is an improvement logistic regression through the addition of two margins for the line which all class points must be behind and the distance to them from the line maximised (again using gradient descent). This ensures that the fairest line out of a set of lines that separate the points is chosen. Support Vector Machines can have the type of margins decided by parameter C, usually divided to either hard one with a large C (narrow with less points out of margin) or one soft with a smaller C (wider with more points out of margin). (Rieck et al., 2012)\n",
    "\n",
    "To help interpreting non linear data for SVMs, kernels (commonly a Raidial Bias Function (RBF)) transform the data to a higher dimension (kernel trick). Some Kernels take extra parameters, for example RBF takes a gamma parameter which controls the variance or influence of the Support Vector Machine (the higher the gamma the lower the variance) (“RBF SVM parameters — scikit-learn 0.20.2 documentation,” n.d.). \n",
    "\n",
    "#### Construction and Tuning \n",
    "#### Interpretation\t\t\t\t\t\n",
    "#### Detailed assessment\n",
    "\n",
    "### Convoluted Neural Network (CNN)\n",
    "\n",
    "#### Introduction \t\t\n",
    "\n",
    "Convoluted neural networks refer to neural networks where the input image (intermediate images from processing steps) are convolved using kernels in successive layers. As previously stated in the SVM introduction kernels help with interpreting non-linear data \n",
    "\n",
    "#### Construction and Tuning \n",
    "#### Interpretation\t\t\t\t\t\n",
    "#### Detailed assessment\n",
    "\n",
    "\t\n",
    "## Summary of Modelling\n",
    "\n",
    "\n",
    "## Evaluation of Data Mining Process  \n",
    "\n",
    "\n",
    "### Business success criteria\n",
    "\t\t\t\t\n",
    "### Review of project success: \n",
    "\n",
    "### Techniques and tools \n",
    "\n",
    "### Future Work \n",
    "\n",
    "\n",
    "## References \n",
    "\n",
    "1. James, G., Witten, D., Hastie, T., Tibshirani, R., 2013. Classification, in: James, G., Witten, D., Hastie, T., Tibshirani, R. (Eds.), An Introduction to Statistical Learning: With Applications in R, Springer Texts in Statistics. Springer New York, New York, NY, pp. 127–173. https://doi.org/10.1007/978-1-4614-7138-7_4\n",
    "\n",
    "2. Rieck, K., Sonnenburg, S., Mika, S., Schäfer, C., Laskov, P., Tax, D., Müller, K.-R., 2012. Support Vector Machines, in: Gentle, J.E., Härdle, W.K., Mori, Y. (Eds.), Handbook of Computational Statistics: Concepts and Methods, Springer Handbooks of Computational Statistics. Springer Berlin Heidelberg, Berlin, Heidelberg, pp. 883–926. https://doi.org/10.1007/978-3-642-21551-3_30\n",
    "\n",
    "3. RBF SVM parameters — scikit-learn 0.20.2 documentation [WWW Document], n.d. URL https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html (accessed 1.19.19).\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "\n",
    "Serveral modelling techniques are applied and tuned. Related to data peroration (different methods have different requirements)\n",
    "\n",
    "## Background \n",
    "\n",
    "outlines the modeling undertaken and its relationship to the data mining goals.\n",
    "\n",
    "## Modeling assumptions\n",
    "\n",
    "explicit assumptions made about the data and any assumptions that are implicit in the modeling technique to be used.\n",
    "\n",
    " \n",
    "## Test design\n",
    "\n",
    "how the models are built, tested, and evaluated: \n",
    "    * Type of model and the training data to be used\n",
    "\t* how the model will be tested or assessed (test data)\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t\n",
    "## Model description and assessment\n",
    "\n",
    "describes the delivered models and overviews the process by which they were produced.\n",
    "\n",
    "\t\t\t\t\n",
    "### Overview of models produced.\n",
    "\n",
    "#### Type of model and relationship to data mining goals\n",
    "\t\t\t\t\t \n",
    "#### Tuning and parameters\n",
    "\t\t\t\t\t\n",
    "#### Detailed description of the model and any special features.\n",
    "\n",
    "Description of the model’s behavior and interpretation\n",
    "\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\n",
    "### Overview of assessment process and results, including any deviations from the test plan. For each model:\n",
    "\n",
    "#### Detailed assessment, including measurements such as accuracy and interpretation of behavior \n",
    "\t\t\t\t\t\t\t\t\t\t \n",
    "#### Insights into why a certain modeling technique and certain parameter settings led to good/bad results \n",
    "#### Conclusions regarding patterns in the data (interpretation)\n",
    "\t\n",
    "\t\t\t\n",
    "## Summary of conclusions\n",
    "\n",
    "\n",
    "\t \n",
    "# Evaluation: \n",
    "\n",
    "Evaluate models and review the steps executed to create them, to be certain the model properly achieves the business objectives. A key objective is to determine if there is some important business issue that has not been sufficiently considered. At the end of this phase, a decision on the use of the data mining results should be reached.\n",
    "\n",
    "\t\t\t\n",
    "## Assessment of data mining results with respect to business success criteria\n",
    " \n",
    "### Review of business objectives and business success criteria (which may have changed during and/or as a result of data mining).\n",
    "\t\t\t\t\t\n",
    "#### For each business success criterion:\n",
    "\t\t\t\t\t\t\n",
    "Detailed comparison between success criterion and data mining results\n",
    "\t\t\t\t\t\t\n",
    "Conclusions about achievability of success criterion and suitability of data mining process\n",
    "\t\t\t\t\n",
    "### Review of project success: \n",
    "\n",
    "\t\t\t\t\t\n",
    "#### Has the project achieved the original business objectives? \n",
    "\t\t\t\t\t\n",
    "#### Are there new business objectives to be addressed later in the project or in new projects?\n",
    "\t\t\t\t\t\n",
    "Conclusions for future data mining projects\n",
    "\t\t\t\n",
    "### Review of process\n",
    "\n",
    "assesses the effectiveness of the project and identifies any factors that may have been overlooked that should be taken into consideration if the project is repeated.\n",
    "\t\t\n",
    "### List of possible actions: make recommendations regarding the next steps in the project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
